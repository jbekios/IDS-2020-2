{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WU6aIq8hhOo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1-5X9OUkA-C2Ih1gOS9Jd7GmkTWUEpDg1)\n",
    "\n",
    "# Laboratorio 05: Clasificación lineal y no lineal\n",
    "\n",
    "## Introducción a Data Science\n",
    "   \n",
    "<center>\n",
    "    <img src='images/national_language.jpg'style=\"width: 300px;\">\n",
    "</center>\n",
    "\n",
    "**Profesor**: Juan Bekios Calfa\n",
    "\n",
    "**Carreras**: ICCI, IECI e IenCI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descripción del problema\n",
    "\n",
    "Se desea construir un clasificador que permita predecir la emoción con que fue escrito un texto. \n",
    "\n",
    "Para esto se contará con una base de datos de películas con sus respectivas revisiones. \n",
    "\n",
    "Cada texto contiene una etiqueta que la define como una revisión positiva o negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Base de datos\n",
    "\n",
    "*  El conjunto de datos que vamos a utilizar para este laboratorio puede ser descargado de _Cornell Natural Language Processing Group_. \n",
    "*  El conjunto de datos consiste en un total de **2000 documentos**. La mitad de los documentos contienen **reseñas positivas** sobre una película, mientras que la otra mitad contiene **reseñas negativas**. \n",
    "*  Más detalles sobre el conjunto de datos se pueden encontrar en este enlace: http://www.cs.cornell.edu/people/pabo/movie-review-data/poldata.README.2.0.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Análisis de sentimientos con Scikit-Learn\n",
    "\n",
    "Una vez descargados los datos, debemos realizar un conjunto de pasos para poder construir el modelo de manera adecuada:\n",
    "\n",
    "1. Importar las liberías.\n",
    "2. Importar la base de datos.\n",
    "3. Acondicionar el texto.\n",
    "4. Convertir el texto a un formato que pueda aprender el modelo.\n",
    "5. Entrenar el modelo.\n",
    "6. Evaluar el modelo.\n",
    "7. Guardar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Importar las librerías\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWwpWkhQhc7X",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # Librería para trabajar con matrices\n",
    "import re          # Librería de expresiones regulares\n",
    "import nltk        # Librería para procesar texto\n",
    "from sklearn.datasets import load_files # Libreria para cargar los textos de un directorio\n",
    "nltk.download('stopwords')              # Descargar palabras no deseadas\n",
    "nltk.download('wordnet')                # Bajar diccionario wordnet\n",
    "import pickle                           # Librería para serializar objetos\n",
    "from nltk.corpus import stopwords       # Módulo para trabajar con stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Importando la base de datos\n",
    "\n",
    "* Utilizaremos la función _load_files_ de la biblioteca sklearn_datasets para importar el conjunto de datos a nuestra aplicación. \n",
    "* La función _load_files_ divide automáticamente el conjunto de datos en datos de entrada y etiquetas. \n",
    "* Por ejemplo, colocamos la ruta  **\"txt_sentoken\"**. La función _load_files_ tratará cada carpeta dentro de la carpeta \"txt_sentoken\" como una categoría y a todos los documentos dentro de esa carpeta se les asignará su categoría correspondiente.\n",
    "* El resultado se entrega como una lista: Una contiene los textos de la revisión y otra la etiqueta (1-Neg y 0-Pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "movie_data = load_files(r\"txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target\n",
    "\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Acondicionar el texto\n",
    "\n",
    "* Una vez que se ha importado el conjunto de datos, el siguiente paso es preprocesar el texto. \n",
    "* El texto puede contener números, caracteres especiales y espacios no deseados. Dependiendo del problema que enfrentemos, es posible que necesitemos o no eliminar estos caracteres especiales y números del texto. \n",
    "* En el laboratorio **eliminaremos todos los caracteres especiales, números y espacios no deseados** del texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remover todos los caracteres especiales\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # Remover todos los caracteres individuales\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remover todos las caracteres individuales que están en el inicio\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Sustituir multiples espacios por uno solo\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Remover prefijp 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Convertir todo a minúsculas\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatización\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Convertir el texto a un formato que pueda aprender el modelo.\n",
    "\n",
    "Las máquinas, a diferencia de los humanos, no pueden comprender el **texto en bruto**. Las máquinas solo pueden ver **números**. En particular, las técnicas estadísticas como el aprendizaje automático solo pueden tratar con números. Por lo tanto, necesitamos convertir nuestro texto en números.\n",
    "\n",
    "Existen dos aproximaciones para convertir el texto en una estructura entendible para los algoritmos: \n",
    "* _The bag of word model_\n",
    "* _Word embedding model_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1 _Bag of Words_\n",
    "\n",
    "El modelo \"bolsa de palabras\" (del inglés, _Bag of Words_) es un método que se utiliza en el procesado del lenguaje para representar documentos ignorando el orden de las palabras. \n",
    "\n",
    "En este modelo, cada documento parece una bolsa que contiene algunas palabras. Por lo tanto, este método permite un modelado de las palabras basado en **diccionarios**, donde cada bolsa contiene unas cuantas palabras del diccionario. \n",
    "\n",
    "\n",
    "ref: https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Parámetros\n",
    "# max_features: Cantidad de features que serán seleccionadas, se escogen las que ocurren más.\n",
    "# min_df: Cantidad mínima de documentos que contienen la palabra.\n",
    "# max_df: Debemos incluir solo aquellas palabras que aparecen en un máximo del 70% de todos los documentos.\n",
    "# stop_words: Diccionario de palabras que son serán consideradas en la vectorización\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2 TF-IDF (_Term frequency_ – _Inverse document frequency_)\n",
    "\n",
    "El valor de **TF-IDF** para una palabra en un documento en particular es mayor si la frecuencia de aparición de esa palabra es mayor en ese documento específico pero menor en todos los demás documentos.\n",
    "\n",
    "* _**T**erm **F**requency_: $\\frac{\\#\\; de\\; ocurrencias\\; de\\; una\\; palabra}{Total\\; de\\; palabras\\; en\\; el\\; documento}$\n",
    "* _**I**nverse **D**ocument **F**requency_: $log \\left( \\frac{Total\\; de\\; documentos}{\\#\\; de\\; documentos\\; que\\; contienen\\; la\\; palabra}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "También se puede hacer directamente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.Entrenar el modelo.\n",
    "\n",
    "### 5.1 Preparar conjunto de entrenamiento y pruebas\n",
    "\n",
    "Para entrenar primero dividiremos el conjunto de datos en un conjunto de **entrenamiento** y otro de **pruebas**.\n",
    "\n",
    "* 80% para el conjunto de entrenamiento.\n",
    "* 20% para el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2 Entrenamiento del modelo de clasificación y predicción de sentiemientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Evaluar el modelo (I)\n",
    "\n",
    "Para evaluar el desempeño de un modelo de clasificación como el que acabamos de entrenar, podemos usar métricas como la matriz de confusión, `la medida F1` y la `precisión`.\n",
    "\n",
    "<center>\n",
    "    <img src='images/matriz-confusion.png'style=\"width: 400px;\">\n",
    "</center>\n",
    "\n",
    "Para encontrar estos valores, podemos usar las utilidades class_report, confusion_matrix y precision_score de la biblioteca `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Evaluar el modelo (II)\n",
    "\n",
    "Para evaluar el desempeño de un modelo de clasificación como el que acabamos de entrenar, podemos usar métricas como la matriz de confusión, `la medida F1` y la `precisión`.\n",
    "\n",
    "* $Exactitud=\\frac{VP+VN}{Total}$\n",
    "* $\\text{Precisión}=\\frac{VP}{\\text{Total clasificados positivos (VP+FP)}}$\n",
    "\n",
    "\n",
    "\n",
    "Para encontrar estos valores, podemos usar las utilidades class_report, confusion_matrix y precision_score de la biblioteca `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Guardar el modelo\n",
    "\n",
    "Podemos guardar el modelo generado como un objeto `pickle` en Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###  Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Probar el modelo cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega de laboratorio\n",
    "\n",
    "**Fecha**: 30/Dic/2020 hasta las 23:00 horas.\n",
    "\n",
    "Se pide:\n",
    "* Evaluar el clasificador lineal `SGDClassifier` con función de pérdida `hinge` y `regresión logística`. Pruebe el clasificador con regularización L1 y L2, y sin regularización. Utilice validación cruzada 5-_fold_ estratificada.\n",
    "* Evaluar un clasificador lineal SVM con kernel: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py. Pruebe diferentes parámetros como en el ejemplo.\n",
    "* Compare el rendimiento de ambos algoritmos e indique cual a su criterio es el mejor y el peor.\n",
    "* Utilice la plantilla de google colab que se encuentra en el campus virtual para realizar el informe."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "modelos_no_lineales.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
